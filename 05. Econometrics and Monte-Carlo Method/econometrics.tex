%\documentclass[14pt,a4paper]{extarticle}
\documentclass[12pt,a4paper]{article} \usepackage[warn] {mathtext}
\usepackage[cp1251]{inputenc} \usepackage[english, russian]{babel}
\usepackage{amsmath} \usepackage{amssymb} \usepackage[top=2cm, bottom=2cm, marginparwidth=0pt, left=3cm, right=1.5cm]{geometry}
\usepackage{wasysym}
\usepackage{tikz}
\usepackage{pgfplots}


\newtheorem{theorem}{Теорема}[section]
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{proposition}[theorem]{Утверждение}
\newtheorem{corollary}[theorem]{Следствие}

\newenvironment{proof}[1][Доказательство.]{\begin{trivlist} \item[\hskip
\labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Определение.]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Пример.]{\begin{trivlist} \item[\hskip
\labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Замечание.]{\begin{trivlist} \item[\hskip
\labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{problem}[1][Задача.]{\begin{trivlist} \item[\hskip
\labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else \ifdim\lastskip<1.5em
\hskip-\lastskip \hskip1.5em plus0em minus0.5em \fi \nobreak \vrule
height0.75em width0.5em depth0.25em\fi}

\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}

\begin{document}

\section{Эконометрика. Методы оценки параметров моделей}

{\bf Эконометрика} описывается как дисциплина, целью которой является наполнение
эмпирическим содержимым экономических отношений. Главным средством достижения
этой цели является применение математических и статистических методов к исследованию
экономических данных.

Термин <<данные>> очень широко используется в самых разных областях человеческой
деятельности. В широком смысле он означает фактический материал, который является 
основой для анализа и принятия решений. С точки зрения статистики, данные --- это информация,
пригодная для анализа и интерпретации. Часто в качестве синонима к слову <<данные>>
используют <<наблюдения>>, т.е. реализации некоторой случайной величины.

Данные получают путем измерения. Формально, измерение --- это приписывание изучаемым
объектам набора символов по определенным правилам. Такое широкое определение необходимо,
поскольку случайные величины, интересующие исследователя, могут иметь различную природу.
Например, для каждого жителя страны можно измерить такие показатели, как: пол, социальное 
положение, температура, рост. Сравнение двух людей по первому показателю может лишь показать,
что их пол одинаков или различен. Для второго показателя можно указать, что эти люди отличны и, 
более того, кто из них занимает более высокое положение в обществе. Третий показатель, помимо
отличий и порядка между людьми, показывает, на сколько они отличаются. Наконец, четвертый показатель
позволяет указать, что один человек выше другого во сколько-то раз, т.е. наличествует некая абсолютная
 точка отсчета. Эти четыре случайные величины представляют примеры четырех различных шкал.

{\bf Номинальная шкала} используется для классификации объектов, она лишь указывает, какому
классу он принадлежит. Разумеется, арифметические действия на такой шкале невозможны.

{\bf Порядковая шкала} используется для классификации и упорядочивания объектов. 

{\bf Интервальная шкала} позволяет количественно оценить разницу между объектами. Для этого
вводится единица измерения и начало отсчета. Структура этой шкалы инвариантна к
линейному преобразованию случайной величины.

{\bf Шкала сравнений}, как и интервальная, количественно оценивает различие, однако начало отсчета
в ней абсолютное, обусловленное природой случайной величины (в приведенном примере рост не может быть
меньше нуля). Структура этой шкалы не меняется при <<растягивании>> или <<сжатии>> шкалы, т.е.
масштабировании единицы измерения.

\subsection{Структура экономических данных}
\subsubsection{Перекрестные данные}
\subsubsection{Временные ряды}
\subsubsection{Объединенные перекрестные данные}
\subsubsection{Панельные (продольные) данные}

\subsection{Регрессионный анализ. Метод наименьших квадратов для оценки линейной регрессии.}
{\bf Регрессионный анализ} — совокупность методов исследования связи одной или
нескольких переменных, называемых зависимыми (регрессандами), с несколькими
другими, называемыми независимыми (регрессорами).
Связь эта представляется в виде математической модели, т.е. уравнения, которое
связывает зависимую переменную с независимыми с учетом множества предположений.
Обычно предполагается что эта зависимость (функция регрессии) известна с
точностью до набора параметров. Модель, функция регрессии которой линейна относительно
параметров модели, также называется линейной регрессионной моделью.
Типичными задачами, которые решаются регрессионным анализом, являются:
\begin{itemize}
	\item получение наилучших точечных и интервальных оценок неизвестных параметров регрессии;
	\item проверка гипотез относительно этих параметров; 
	\item проверка адекватности предполагаемой модели;
	\item проверка соблюдения предположений, принятых при составлении модели;
	\item получение наилучших точечных и интервальных оценок значений зависимой переменной (прогноз).
\end{itemize}
Регрессионная модель позволяет, во-первых, описать зависимость между интересующими  исследователя
переменными, вскрыть или прояснить структуру причинных связей. Во-вторых, она может
служить основой для прогноза поведения зависимой переменной, что очень важно, когда
прямые измерения либо невозможны, либо  экономически невыгодны.\\\\

{\bf Introductory conometrics. Matrix form of linear regression}\\\\

\subsection{Нелинейная регрессия и метод наименьших квадратов}

Пусть теперь набор данных $y_1, y_2, \dots, y_n \in \mathbb{R}$, измеренный при известных
значениях $k$ факторов (регрессоров) $x_1, x_2, \dots, x_k $  мы пытаемся аппроксимировать
нелинейной функцией регрессии $\varphi(x_1, \dots, x_k; \alpha)$ так, чтобы
\[
	y_t = \varphi_t(x_{t1}, x_{t2}, \dots, x_{tk}; \alpha) + \varepsilon_t~.
\]
Полагая значения $x_{ti}$ заданными и фиксированными, применим метод наименьших квадратов
для нахождения набора параметров $\alpha \in \mathbb{R}^m$, дающего наилучшее приближение.
Введя обозначение $\varphi_t(x_{t1}, x_{t2}, \dots, x_{tk}; \alpha) = f_t(\alpha), t = \overline{1,n}$ и 
составив сумму квадратов невязок, получаем задачу математического программирования вида
\[
	Q(\alpha) = \sum\limits_{t=1}^n{\left(y_t - f_t(\alpha)\right)^2} \longrightarrow \min_{\alpha}
\]
Заметим, что случайные погрешности $\varepsilon_t$ в уравнении нелинейной регрессии предполагаются
имеющими нулевое математическое ожидание и постоянную дисперсию, и не коррелирующими между собой.

Вообще говоря, для нелинейной регрессии минимум суммы квадратов невязок может и не достигаться.
Кроме того, задача математического программирования может быть многоэкстремальной. Однако за многие годы использования и
развития МНК эти проблемы неплохо изучены и существуют теоретические критерии, позволяющие устанавливать
корректность поставленной задачи регрессии. К сожалению, проверка выполнения этих критериев часто представляет
собой очень сложную задачу и до конца может быть проведена только в редких случаях.

\subsubsection{Минимизация суммы квадратов методом Ньютона-Гаусса}
Пусть $\alpha_0$ --- некоторое достаточно хорошее приближение к оптимальному значению $\alpha$.
Метод Ньютона минимизации функций многих переменных строит последовательность точек по правилу
\[
	\alpha_{s+1} = \alpha_s - \left(\frac{\partial^2 Q(\alpha^s)}{\partial \alpha^2}\right)^{-1}\frac{\partial Q(\alpha_s)}{\partial \alpha},
		\qquad s = 0, 1, \dots
\]
Особая форма минимизируемой функции позволяет модифицировать эту формулу. Во-первых, гессиан  $Q(\alpha)$ имеет вид
\[
	\frac{1}{2}\frac{\partial^2 Q}{\partial \alpha^2} = \left(\frac{\partial f_i}{\partial \alpha_j}\right)^T\left(\frac{\partial f_i}{\partial \alpha_j}\right) - 
	\sum\limits_{t=1}^n{\left(y_t - f_t(\alpha)\right)\frac{\partial^2 f_t(\alpha)}{\partial \alpha^2}}~.
\]
Во-вторых, поскольку $\alpha_0$ близка к точке минимума, невязки $y_t - f_t(\alpha)$ в некоторой ее окрестности будут невелики.
В-третьих, если предположить, что функция регрессии <<не очень нелинейна>>, то $\displaystyle \frac{\partial^2 f_t(\alpha)}{\partial \alpha^2}$
тоже будут малы. 

Учитывая это, суммой в гессиане $Q(\alpha)$ можно пренебречь, т.е.
\[
	\frac{\partial^2 Q}{\partial \alpha^2} \approx 2\left(\frac{\partial f_i}{\partial \alpha_j}\right)^T\left(\frac{\partial f_i}{\partial \alpha_j}\right)
\]
Введя матрицу первых производных $\displaystyle F(\alpha) = \left(\frac{\partial f_i(\alpha)}{\partial \alpha_j}\right)$, можем
записать
\[
	\frac{\partial^2 Q}{\partial \alpha^2} \approx 2F^T(\alpha)F(\alpha), \qquad \frac{\partial Q}{\partial \alpha} = -2F^T(\alpha)(y - f(\alpha))~.
\]
Формулу итераций метода Ньютона можно теперь записать в виде
\[
	\alpha_{s+1} = \alpha_s + \left(F^T(\alpha_s)F(\alpha_s)\right)^{-1}F^T(\alpha_s)(y - f_(\alpha_s)), \qquad s = 0, 1, \dots \eqno(*)
\]
Очевидно, в последней формуле, как и в исходной, предполагается, что $\operatorname{rank}(F(\alpha)) = m$.

\begin{problem}
	Как еще можно получить формулу $(*)$? 
\end{problem}

Формула $(*)$ является основой метода Ньютона-Гаусса нахождения параметров нелинейной регрессии.
Основной недостаток метода --- сходимость гарантируется при довольно жестких условиях. Если эти условия нарушаются
(например, матрица $\displaystyle \frac{\partial^2 f_t(\alpha)}{\partial \alpha^2}$ сильно отлична от нулевой и/или невязки
$\abs{y_t - f_t(\alpha)} \gg 0$ ), последовательность приближений ${\alpha_s}$ расходится. Простейший выход из ситуации
заключается в регулировке длины шага:
\[
	\alpha_{s+1} = \alpha_s + \lambda_s p_s,
\]
где $\displaystyle p_s = \left(F^T(\alpha_s)F(\alpha_s)\right)^{-1}F^T(\alpha_s)(y - f_(\alpha_s))$ --- направление минимизации,
(такое же, как и в исходном методе), а $\lambda_s > 0$ --- длина шага в этом направлении. Очевидный метод регулировки 
шага --- метод редукции --- позволяет строить строго релаксационные последовательности, т.е. такие, что
\[
	Q(\alpha_{s+1}) < Q(\alpha_s)~.
\]
Метод редукции состоит в следующем: выберем два параметра метода, $0 < \tau < 1$ и $0 < \varkappa < 1$.
Далее, на каждой итерации 
\begin{enumerate}
	\item Полагаем $\lambda = 1$.
	\item Проверяем неравенство
\[
		Q(\alpha_s + \lambda p_s) - Q(\alpha_s) < -\tau q^T_sp_s, \eqno(**)
\]
	где $\displaystyle q_s = -\frac{\partial Q(\alpha_s)}{\partial \alpha}, q_s \neq 0$.
	\item Если неравенство выполняется, полагаем $\lambda_s = \lambda$. В противном случае
	уменьшаем шаг в $\varkappa$ раз, т.е. $\lambda := \varkappa \lambda$ и возвращаемся к шагу 2.
\end{enumerate}
Можно показать, что, если $q_s \neq 0$, то для любого $0 < \tau < 1$ найдется такое $\bar{\lambda}$, что для
всех $0 < \lambda < \bar{\lambda}$ неравенство $(**)$ выполняется.

\subsubsection{Минимизация суммы квадратов методом Левенберга-Марквардта}
Помимо условий сходимости, еще одним ограничением на применение метода Ньютона
является условие полного ранга матрицы производных функции регрессии: $\operatorname{rank}(F(\alpha)) = m$.
Рассмотрим, например, модель нелинейной регрессии вида
\[
	y_t = \alpha_1 e^{\alpha_2 x_t} + \alpha_3 e^{\alpha_4 x_t} + \varepsilon_t~.
\]
Легко проверить, что на подпространстве (линейном) $\alpha_2 = \alpha_4$ не выполняется
условие $\abs{F^T(\alpha)F(\alpha)} > 0$.
\begin{problem}
	Проверьте последнее утверждение.
\end{problem}
Даже если условие формально выполняется, определитель матрицы может быть настолько мал, что из-за погрешности
вычислений обратная матрица $\left(F^T(\alpha)F(\alpha)\right)^{-1}$ не будет положительно определенной. Это может привести,
в частности, к зацикливанию итераций. На практике маловероятно, что $\operatorname{rank}(F(\alpha)) < m$, однако
плохая обусловленность матрицы может представлять реальную проблему. Один из методов, который ее обходит ---
метод Левенберга-Марквардта.

В 1944 г. Кеннет Левенберг предложил использовать простой прием, который обеспечивал бы
положительную определенность матрицы в итерациях метода Ньютона-Гаусса. Прием этот заключался в
добавлении матрицы, пропорциональной единичной следующим образом:
\[
	\alpha_{s+1} = \alpha_s + \left(F^T(\alpha_s)F(\alpha_s) + \mu_s I\right)^{-1}F^T(\alpha_s)(y - f_(\alpha_s)), \qquad s = 0, 1, \dots
\]
Прием имел простое эмпирическое обоснование: матрица $F^T(\alpha_s)F(\alpha_s)$ неотрицательно определена, поэтому
$F^T(\alpha_s)F(\alpha_s) + \mu_s I$ определена положительно ($\mu_s > 0$). Следовательно, направление минимизации будет составлять
острый угол с направлением антиградиента минимизируемой функции. Эта идея была развита двадцать лет спустя, когда в 1963 г. Дональд
Марквардт предложил вместо единичной матрицы использовать диагональную $D = \operatorname{diag}\left(F^T_sF_s\right)$,
что равносильно  масштабированию направления минимизации. Для улучшения сходимости метода можно использовать переменный
шаг, подбирая его методом редукции. Расчетная формула метода Левенберга-Марквардта имеет вид
\[
	\alpha_{s+1} = \alpha_s + \lambda_s\left(F^T(\alpha_s)F(\alpha_s) + \mu_s D_s\right)^{-1}F^T(\alpha_s)(y - f_(\alpha_s)), \qquad s = 0, 1, \dots
\]
Самым сложным вопросом при использовании этого метода является выбор $\mu_s$. Д.~Марквардтом было показано, что при больших значениях $\mu_s$
алгоритм близок к градиентному, при малых --- к методу Ньютона-Гаусса. Поскольку последний хорошо сходится только при наличии
достаточно близкого к точке минимума начального приближения, а градиентный метод, наоборот, плохо ведет себя в <<овраге>>, разумной представляется
следующая общая рекомендация: значения $\mu_s$ следует постепенно уменьшать, устремляя к нулю.

\subsection{Метод максимального правдоподобия. Оценка линейной модели}

С точки зрения статистики, набор измерений регрессанда $(y_1, y_2, \dots, y_n)$ является случайной выборкой
из генеральной совокупности с неизвестными свойствами. И задача восстановления зависимости от регрессоров
эквивалентна восстановлению этих свойств: наилучшей моделью зависимости будет та, которой больше всего
соответствуют имеющиеся данные.

Свойства случайной величины, как известно, описываются функцией распределения вероятностей и/или
функцией плотности вероятностей. Как правило, справедливо предположение, что конкретная зависимость
описывается параметрическим семейством плотностей вероятностей, т.е. каждому значению некоторого 
набора параметров соответствует определенная функция плотности вероятности. Таким образом, восстановление
зависимости сводится к оценке параметров плотности вероятности, наилучшим образом соответствующей измерениям.

Предположим, что $(y_1, y_2, \dots, y_n)$ --- независимые и одинаково распределенные реализации некоторой случайной
величины. Обозначим истинную плотность вероятности этой величины через $f_0(y)$. Пусть эта функция принадлежит
параметризованному семейству $\left\{ f(\cdot | \theta) : \theta \in \Omega \right\}$, так, что $f_0(y) = f(y | \theta_0)$.
Значение  $\theta_0$ (вообще говоря --- векторного) будем называть истинным значением параметра распределения.
Задача состоит в том, чтобы найти оценку $\hat{\theta}$, наиболее близкую к $\theta_0$.

Поскольку измерения $y_t$ предполагаются независимыми, вероятность реализации всех $n$ значений равна произведению
вероятностей реализации каждого из них. Поэтому
\[
	f\left(y_1, y_2, \dots, y_n | \theta\right) = \prod\limits_{t=1}^n {f(y_t | \theta)}~.
\]
Теперь, поскольку набор $(y_1, y_2, \dots, y_n)$ действительно реализовался, можно смотреть на посленее выражение
как на функцию $\theta$ при фиксированных $y_t$. Эта функция называется функцией правдоподобия:
\[
	L \left(\theta | y_1, y_2, \dots, y_n\right) = f\left(y_1, y_2, \dots, y_n | \theta\right) = \prod\limits_{t=1}^n {f(y_t | \theta)}~.
\]
На практикечасто используется так называемая логарифмическая функция правдоподобия:
\[
	\ln L \left(\theta | y_1, y_2, \dots, y_n\right) =  \sum\limits_{t=1}^n {\ln{f(y_t | \theta)}}
\]
или ее масштабированный вариант:
\[
	l\left(\theta | y_1, y_2, \dots, y_n\right) =  \frac{1}{n}\sum\limits_{t=1}^n {\ln{f(y_t | \theta)}}~.
\]

Оценкой метода максимального правдоподобия называется решение задачи
\[
	\hat{\theta}_{MLE} = \arg\max_{\theta \in \Omega}{L \left(\theta | y_1, y_2, \dots, y_n\right)},
\]
где вместо функции правдоподобия $L$ могут быть использованы любые функции, получаемые из нее монотонным преобразованием,
так как абсолютные значения функции правдоподобия не важны.
%\begin{center}
%\begin{tikzpicture}
%
%%	\draw[line width=1.5pt] [->] (0, 0) -- (0, 6);	
%%	\draw (0.5, 5.5) node {$f$};
%%	\draw[line width=1.5pt] [->] (0, 0) -- (12, 0);	
%%	\draw (11.5, -0.5) node {$x$};
%%	\addplot [orange,samples=7] {sin(deg(x))};
%	\begin{axis}[domain=0:10, xmin=0, xmax=12]
%		\addplot[red,samples=200] {exp(-0.5*((x-5)^2))/sqrt(2*3.1415926)};
%		\addplot[green,samples=200] {exp(-0.5*(x-6.5)^2)/sqrt(2*3.1415926)};
%	\end{axis}	
%
%\end{tikzpicture}
%\end{center}

\centerline{\bf{<Пример из "Эконометрики" Доугерти>}}

Покажем, как метод максимального правдоподобия позволяет найти оценки параметров линейной регрессии.
Согласно идее метода, предположим, что каждое наблюдение $y_t$ есть реализация случайной величины,
подчиняющейся распределению $y_t \in N(\beta_0 + \beta_1x_t, \sigma^2), t = \overline{1,n}$. Параметрами
распределения являются величины $\beta_0, \beta_1, \sigma^2$. Нетрудно видеть, что
\[
	\begin{array}{lcl}
		f\left(y_1, y_2, \dots, y_n | \beta_0, \beta_1, \sigma^2\right) & = & \displaystyle\prod\limits_{t=1}^n{f\left(y_t| \beta_0, \beta_1, \sigma^2\right)} = \\
		& = & \displaystyle\prod\limits_{t=1}^n{\frac{1}{\sqrt{2\pi\sigma^2}} ~e^{\displaystyle-\frac{\left(y_t - \beta_0 - \beta_1x_t\right)^2}{2\sigma^2}}} \\
		& = & \displaystyle\frac{1}{\left(2\pi\sigma^2\right)^{\frac{n}{2}}}~e^{\displaystyle-\frac{1}{2\sigma^2}\sum\limits_{t=1}^n{\left(y_t - \beta_0 - \beta_1x_t\right)^2}}
	\end{array}
\]
Значит, логарифмическая функция правдоподобия имеет вид
\[
	\ln{L\left(\beta_0, \beta_1, \sigma^2 | y_1, y_2, \dots, y_n\right)} = -\frac{n}{2}\ln{2\pi\sigma^2} - -\frac{1}{2\sigma^2}\sum\limits_{t=1}^n{\left(y_t - \beta_0 - \beta_1x_t\right)^2}
\]
Решение задачи максимизации функции правдоподобия приводит к системе уравнений:
\[
	\left\{\begin{array}{lclcl}
		\displaystyle\frac{\partial \ln{L}}{\partial \beta_0} & = & \displaystyle\frac{1}{\sigma^2}\sum\limits_{t=1}^n{\left(y_t - \beta_0 - \beta_1x_t\right)} & = & 0 \\
		\displaystyle\frac{\partial \ln{L}}{\partial \beta_1} & = & \displaystyle\frac{1}{\sigma^2}\sum\limits_{t=1}^n{\left(y_t - \beta_0 - \beta_1x_t\right)x_t} & = & 0  \\
		\displaystyle\frac{\partial \ln{L}}{\partial \sigma^2} & = & -\displaystyle\frac{n}{\sigma^2} + \displaystyle\frac{1}{\sigma^4}\sum\limits_{t=1}^n{\left(y_t - \beta_0 - \beta_1x_t\right)^2} & = & 0 
	\end{array}\right.
\]
Ее решение легко можно найти:
\[
	\begin{array}{lcl}
		\hat{\beta_1} & = &\displaystyle \frac{\displaystyle\sum\limits_t{\left(x_t - \bar{x}\right)\left(y_t - \bar{y}\right)}}{\displaystyle\sum\limits_t{\left(x_t - \bar{x}\right)^2}} \\
		\hat{\beta_0} & = &\bar{y} - \hat{\beta_1}\bar{x} \\\\
		\hat{\sigma}^2 & = & \displaystyle \frac{1}{n} \displaystyle\sum\limits_t {e^2_t} = \displaystyle \frac{1}{n}\displaystyle\sum\limits_t {\left(y_t - \hat{\beta_0} - \hat{\beta_1}x_t\right)^2}
	\end{array}
\]
Как видно, оценки параметров линейной модели методом максимального правдоподобия совпадают с оценками метода МНК.
\begin{problem}
	Показать, что оценки параметров $\beta_0$ и $\beta_1$ являются в данном случае несмещенными. Найти их дисперсию.
\end{problem}

Оценки метода максимального правдоподобия обладают такими свойствами, как состоятельность и асимптотическая эффективность, причем 
при довольно мягких ограничениях на исходные данные и модель.

\begin{problem}
	Было проведено маркетологическое исследование эффективности рекламной кампании некоторого товара.
	Через равные промежутки времени проводилось тестирование фокус-группы исследуемых, которое выявляло,
	какой объем информации о продукте оставался в актуальной памяти. Для этого испытуемым задавалось 
	$m$ вопросов и измерялась величина $\displaystyle \frac{x_t}{m}$, где $x_t$ --- число верных ответов при тестировании
	в момент времени $t$. Затем она усреднялась по всем испытуемым. Как можно видеть, величина $x_t$ подчиняется
	биномиальному распределению, вопрос только в том, по какому закону меняется вероятность успешного исхода
	в каждом из независимых испытаний.
	
	Очевидно, что вероятность верного ответа со временем уменьшается, т.к. из памяти стираются ненужные детали. 
	Были выдвинуты две гипотезы:
	\[
		\begin{array}{lcl}
			\text{Степенная модель}~H_0 &:& \quad p\left(w, t\right) = w_1 t^{-w_2}, \quad w_1,w_2 > 0 \\
			\text{Экспоненциальная модель}~H_1 &:& \quad p\left(w, t\right) = w_1 e^{-w_2t}, \quad w_1,w_2 > 0 
		\end{array}
	\]
	Необходимо применить метод максимального правдоподобия, чтобы найти оценки параметров $w_1, w_2$ в каждом случае и
	выяснить, какая из гипотез лучше объясняет имеющийся набор данных $y = \left(0.94, 0.77, 0.40, 0.26, 0.24, 0.16\right)^T$, причем
	измерения проводились в моменты времени $t = 1, 3, 6, 9, 12, 18$. Объем испытаний при каждом измерении равен $m = 100$.
	Сравнить графически полученные модели с реальными данными.
\end{problem}


\end{document}











